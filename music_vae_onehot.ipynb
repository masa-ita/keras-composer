{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 69812,
     "status": "ok",
     "timestamp": 1537926930565,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "CMMiOVaV2PD2",
    "outputId": "a80be742-b257-4ad8-88d6-46356dc6ffdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'keras-composer'...\n",
      "remote: Enumerating objects: 177, done.\u001b[K\n",
      "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
      "remote: Compressing objects: 100% (166/166), done.\u001b[K\n",
      "remote: Total 177 (delta 16), reused 171 (delta 10), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (177/177), 1.00 GiB | 16.01 MiB/s, done.\n",
      "Resolving deltas: 100% (16/16), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/masa-ita/keras-composer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ywgkfCR62kTV"
   },
   "outputs": [],
   "source": [
    "!mkdir keras-composer/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32472,
     "status": "ok",
     "timestamp": 1537927027192,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "XiDr7O5V0qgd",
    "outputId": "74a75571-414e-441e-a3c7-de9f8570553a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: music21 in /Users/masatoshi/pyenvs/music/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rV2eYRRzX5J"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import objectives, backend as K\n",
    "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from music21 import converter, instrument, note, chord, stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVQSOQeMzX5Z"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5nk6pTKzX5U"
   },
   "outputs": [],
   "source": [
    "class VAE(object):\n",
    "    def __init__(self, vocab_size=500, max_length=300, latent_rep_size=64):\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.autoencoder = None\n",
    "\n",
    "        x = Input(shape=(max_length, vocab_size))\n",
    "\n",
    "        vae_loss, encoded = self._build_encoder(x, latent_rep_size=latent_rep_size, max_length=max_length)\n",
    "        self.encoder = Model(inputs=x, outputs=encoded)\n",
    "        encoder_out = self.encoder(x)\n",
    "\n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "\n",
    "        decoded = self._build_decoder(encoded_input, vocab_size, max_length)\n",
    "        self.decoder = Model(encoded_input, decoded)\n",
    "        \n",
    "        decoder_out = self.decoder(encoder_out)\n",
    "\n",
    "        self.autoencoder = Model(inputs=x, outputs=decoder_out)\n",
    "        self.autoencoder.compile(optimizer='Adam',\n",
    "                                 loss=vae_loss,\n",
    "                                 metrics=['accuracy'])\n",
    "        \n",
    "    def _build_encoder(self, x, latent_rep_size=64, max_length=300, epsilon_std=0.01):\n",
    "        h = LSTM(512, return_sequences=False, name='lstm_1')(x)\n",
    "        h = Dropout(0.2)(h)\n",
    "        h = Dense(256, activation='relu', name='dense_1')(h)\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_rep_size), mean=0., stddev=epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation='linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation='linear')(h)\n",
    "    \n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')([z_mean, z_log_var]))\n",
    "\n",
    "    def _build_decoder(self, encoded, vocab_size, max_length):\n",
    "        repeated_context = RepeatVector(max_length)(encoded)\n",
    "    \n",
    "        h = LSTM(512, return_sequences=True, name='dec_lstm_1')(repeated_context)\n",
    "    \n",
    "        decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)\n",
    "    \n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gwaMIKmxzX5f"
   },
   "outputs": [],
   "source": [
    "def parse_midi_files():\n",
    "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
    "    notes = []\n",
    "    songs = []\n",
    "\n",
    "    for file in glob.glob(\"midi_songs/*.mid\"):\n",
    "        song = []\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "        print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                song.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                song.append('.'.join(str(n) for n in element.normalOrder))\n",
    "        songs.append(song)\n",
    "        notes += song\n",
    "\n",
    "    return notes, songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22243,
     "status": "ok",
     "timestamp": 1537927060195,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "IxKWyHIbzX5j",
    "outputId": "ab494ce3-d9fe-4eec-b3fe-0111b5f1ab3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing midi_songs/bwv782.mid\n",
      "Parsing midi_songs/bwv783.mid\n",
      "Parsing midi_songs/bwv781.mid\n",
      "Parsing midi_songs/bwv780.mid\n",
      "Parsing midi_songs/bwv784.mid\n",
      "Parsing midi_songs/bwv785.mid\n",
      "Parsing midi_songs/bwv778.mid\n",
      "Parsing midi_songs/bwv786.mid\n",
      "Parsing midi_songs/bwv779.mid\n",
      "Parsing midi_songs/bwv774.mid\n",
      "Parsing midi_songs/bwv775.mid\n",
      "Parsing midi_songs/bwv777.mid\n",
      "Parsing midi_songs/bwv776.mid\n",
      "Parsing midi_songs/bwv772.mid\n",
      "Parsing midi_songs/bwv773.mid\n"
     ]
    }
   ],
   "source": [
    "notes, songs = parse_midi_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USn-0QIjzX5o"
   },
   "outputs": [],
   "source": [
    "pitchnames = sorted(set(notes))\n",
    "n_vocab = len(pitchnames)\n",
    "\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "int_to_note = dict([[number, note] for note, number in note_to_int.items()])\n",
    "\n",
    "encoded_songs = [[note_to_int[note] for note in song] for song in songs]\n",
    "\n",
    "# songs_text = [' '.join(song) for song in songs]\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=NUM_WORDS, filters='', lower=False)\n",
    "# tokenizer.fit_on_texts(songs_text)\n",
    "# note2code = tokenizer.word_index\n",
    "\n",
    "# songs_codes = tokenizer.texts_to_sequences(songs_text)\n",
    "padded_songs = pad_sequences(encoded_songs, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FyVJ0jEMzX5s"
   },
   "outputs": [],
   "source": [
    "temp = np.zeros((padded_songs.shape[0], MAX_LENGTH, n_vocab))\n",
    "temp[np.expand_dims(np.arange(padded_songs.shape[0]), axis=0).reshape(padded_songs.shape[0], 1), \n",
    "           np.repeat(np.array([np.arange(MAX_LENGTH)]), padded_songs.shape[0], axis=0), padded_songs] = 1\n",
    "\n",
    "one_hot_encoded_songs = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9lB-LcDzX5w"
   },
   "outputs": [],
   "source": [
    "def create_model_checkpoint(dir, model_name):\n",
    "    filepath = dir + '/' + \\\n",
    "               model_name + \"-{epoch:02d}-{acc:.2f}-{loss:.2f}.h5\"\n",
    "    directory = os.path.dirname(filepath)\n",
    "\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath,\n",
    "                                                              monitor='loss',\n",
    "                                                              verbose=1,\n",
    "                                                              save_best_only=True)\n",
    "\n",
    "    return checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8615689,
     "status": "ok",
     "timestamp": 1537947137087,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "4SeGOLOTzX5y",
    "outputId": "4271bc1f-8576-4cb8-e844-ae69aa6883f0"
   },
   "outputs": [],
   "source": [
    "model = VAE(vocab_size=n_vocab, latent_rep_size=2, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8615689,
     "status": "ok",
     "timestamp": 1537947137087,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "4SeGOLOTzX5y",
    "outputId": "4271bc1f-8576-4cb8-e844-ae69aa6883f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: loss improved from inf to 13.34625, saving model to vae_output/music_vae-01-0.04-13.35.h5\n",
      "\n",
      "Epoch 00002: loss improved from 13.34625 to 12.45865, saving model to vae_output/music_vae-02-0.04-12.46.h5\n",
      "\n",
      "Epoch 00003: loss improved from 12.45865 to 12.38944, saving model to vae_output/music_vae-03-0.03-12.39.h5\n",
      "\n",
      "Epoch 00004: loss improved from 12.38944 to 12.32700, saving model to vae_output/music_vae-04-0.05-12.33.h5\n",
      "\n",
      "Epoch 00005: loss improved from 12.32700 to 12.30000, saving model to vae_output/music_vae-05-0.04-12.30.h5\n",
      "\n",
      "Epoch 00006: loss improved from 12.30000 to 12.26549, saving model to vae_output/music_vae-06-0.05-12.27.h5\n",
      "\n",
      "Epoch 00007: loss did not improve from 12.26549\n",
      "\n",
      "Epoch 00008: loss improved from 12.26549 to 12.26047, saving model to vae_output/music_vae-08-0.05-12.26.h5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a940a753aa92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                                         verbose=0)\n\u001b[0m",
      "\u001b[0;32m~/pyenvs/music/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/pyenvs/music/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenvs/music/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenvs/music/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenvs/music/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpointer = create_model_checkpoint('vae_output', 'music_vae')\n",
    "csv_logger = CSVLogger(os.path.join('vae_output', 'music_vae_log.csv'))\n",
    "\n",
    "history = model.autoencoder.fit(x=one_hot_encoded_songs, \n",
    "                                                        y=one_hot_encoded_songs,\n",
    "                                                        batch_size=1, \n",
    "                                                        epochs=8000, \n",
    "                                                        callbacks=[checkpointer, csv_logger], \n",
    "                                                        verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTEYK2STzX53"
   },
   "outputs": [],
   "source": [
    "prediction_output = model.autoencoder.predict(songs_one_hot[np.newaxis, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1537890383742,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "Kn_6yV1RzX56",
    "outputId": "65dd3169-4d32-4cdc-a25a-d83df81aa612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5.11', '1.4', '2.5', '2.5', '4.7', '9.1', 'B-3', 'D5', 'E3', 'E5', 'F3', 'D5', 'C#5', 'G3', 'D5', 'E5', 'A3', 'F5', 'E5', 'F5', 'E5', 'F5', 'A2', 'E5', 'D5', 'D3', 'D5', 'C5', 'B-4', 'A4', '10.2', '0.4', '5.9', '7.10', '9', '7.10', '5.7', 'C4', 'E4', 'B-3', '0.2', 'C3', 'C5', 'B-4', 'F3', 'A4', 'G4', '3.9', '11.2', 'E-3', 'C5', 'B4', 'F3', 'C5', 'G3', 'E-3', 'F3', '3.9', '11.2', '0', 'B2', 'D5', 'C5', 'G#3', 'D5', 'G3', 'F3', 'E-3', '2.7', '9.0', '11.2', '0.3', '2.5', '3.7', '8.0', 'B2', 'D5', 'A2', '11.0', 'G2', 'B4', 'A4', 'G4', 'F5', 'G2', 'E-5', 'D5', '8.0', '7.10', '5.8', '7', 'G#2', 'F4', 'E-5', 'A2', 'D5', 'C5', '7.10', '5.9', '3.7', '7.9', 'B-2', 'F5', 'E-5', 'B2', 'D5', 'F5', '0.3', '2.7', '9.0', '11.2', '0.3', '2.5', '3.7', '8.0', 'D3', 'B4', 'A4', 'B4', 'C3', 'B2', 'D5', 'A2', 'G2', 'F3', 'E-3', 'F4', 'D3', 'C3', 'E-4', 'B-2', 'G#2', 'C5', 'G2', 'F2', 'D4', 'E-2', 'F2', 'C5', 'B4', 'G2', 'A4', 'B4', '0', '2.7', '0.3', '2.5', 'G3', 'E-5', 'D5', 'G#3', 'E-5', 'B-3', 'G3', 'G#3', '7.0', '2.5', '3', 'D3', 'F5', 'E-5', 'C4', 'F5', 'B-3', 'G#3', 'G3', 'F3', '3.7', '9.2', 'B-3', 'G5', 'F5', 'C4', 'G5', 'D4', '5.10', '0.3', '10.2', '9.0', '7.10', '6.9', '3.7', '2.6', '9.0', 'B-3', 'E-4', 'D4', 'E4', 'F#4', 'G4', '9.0', '10.2', '3.7', '6.9', '7', '6.9', '4.7', 'F#3', 'D4', 'C5', 'F3', 'B-4', '5', '4.7', '2.5', '4.7', '2.5', 'E3', 'A4', 'B-4', 'A4', 'B-4', 'E-3', 'B-4', 'D5', 'C5', 'D5', 'C5', 'D5', 'C5', 'D5', 'C5', 'B-4', 'C5', 'A4', 'B-4', 'A4', 'B-4', 'A4', 'B-4', 'A4', 'B-4', 'A4', 'B-4', 'A4', 'B-4', 'A4', 'B-4', 'A4', 'G4', 'A4', 'D4', 'D4', 'D4', 'D4', 'D4', 'D4', 'D4', 'D4', 'D4', 'D4', 'D4', '3.6', 'D5', 'D5', 'D5', 'D5', 'D5', '9.0', '9.0', '9.0', '9.0', 'G3', 'C5', 'E-3', 'C5', 'C5', 'C5', 'C5', '0.3', 'C5', '0.3', 'C5', '0.3', 'A4', 'A2', 'A4', 'A2', 'A4', 'D3', 'A4', 'G4', 'A4', 'G4', 'G4', 'G4', 'G4', 'G4', 'G4', 'G4', 'G4', 'G4', 'G4', 'G4', 'G4', 'G4', 'B-2', 'G4', 'B-2', 'G4']\n"
     ]
    }
   ],
   "source": [
    "prediction_indices = np.argmax(prediction_output, axis=2)\n",
    "code2note = dict([[code, note] for note, code in note2code.items()])\n",
    "\n",
    "prediction_song = [code2note[index] for index in prediction_indices[0]]\n",
    "print(prediction_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Za4ZFVCzX59"
   },
   "outputs": [],
   "source": [
    "def create_midi(prediction_output, file_path):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2BvvQAhzX6A"
   },
   "outputs": [],
   "source": [
    "create_midi(prediction_song, 'test_vae_onehot_out4.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqFvOMhLzX6D"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('test_vae_onehot_out4.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PV0Hg6R17vOj"
   },
   "outputs": [],
   "source": [
    "decoder_input = np.array([[-0.0001, 0.0001]])\n",
    "decoder_predicted = model.decoder.predict(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1537890821462,
     "user": {
      "displayName": "Masatoshi Itagaki",
      "photoUrl": "",
      "userId": "04462290006021813429"
     },
     "user_tz": -540
    },
    "id": "qEuiJTb-bRRa",
    "outputId": "43d46752-aa0f-437b-f4b7-85458529c0e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6', '0.6']\n"
     ]
    }
   ],
   "source": [
    "prediction_indices = np.argmax(decoder_predicted, axis=2)\n",
    "code2note = dict([[code, note] for note, code in note2code.items()])\n",
    "\n",
    "prediction_song = [code2note[index] for index in prediction_indices[0]]\n",
    "print(prediction_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAQv_8aqb_gj"
   },
   "outputs": [],
   "source": [
    "files.download('weights/music_vae-4128-0.90-1.45.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "music_vae_onehot.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/masa-ita/keras-composer/blob/master/music_vae_onehot.ipynb",
     "timestamp": 1537880206798
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
