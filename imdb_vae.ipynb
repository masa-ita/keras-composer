{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras import objectives, backend as K\n",
    "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed\n",
    "from keras.models import Model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(object):\n",
    "    def create(self, vocab_size=500, max_length=300, latent_rep_size=200):\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.sentiment_predictor = None\n",
    "        self.autoencoder = None\n",
    "\n",
    "        x = Input(shape=(max_length,))\n",
    "        x_embed = Embedding(vocab_size, 64, input_length=max_length)(x)\n",
    "\n",
    "        vae_loss, encoded = self._build_encoder(x_embed, latent_rep_size=latent_rep_size, max_length=max_length)\n",
    "        self.encoder = Model(inputs=x, outputs=encoded)\n",
    "\n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "        predicted_sentiment = self._build_sentiment_predictor(encoded_input)\n",
    "        self.sentiment_predictor = Model(encoded_input, predicted_sentiment)\n",
    "\n",
    "        decoded = self._build_decoder(encoded_input, vocab_size, max_length)\n",
    "        self.decoder = Model(encoded_input, decoded)\n",
    "\n",
    "        self.autoencoder = Model(inputs=x, outputs=[self._build_decoder(encoded, vocab_size, max_length), self._build_sentiment_predictor(encoded)])\n",
    "        self.autoencoder.compile(optimizer='Adam',\n",
    "                                 loss=[vae_loss, 'binary_crossentropy'],\n",
    "                                 metrics=['accuracy'])\n",
    "        \n",
    "    def _build_encoder(self, x, latent_rep_size=200, max_length=300, epsilon_std=0.01):\n",
    "        h = Bidirectional(LSTM(500, return_sequences=True, name='lstm_1'), merge_mode='concat')(x)\n",
    "        h = Bidirectional(LSTM(500, return_sequences=False, name='lstm_2'), merge_mode='concat')(h)\n",
    "        h = Dense(435, activation='relu', name='dense_1')(h)\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_rep_size), mean=0., stddev=epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation='linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation='linear')(h)\n",
    "    \n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')([z_mean, z_log_var]))\n",
    "\n",
    "    def _build_decoder(self, encoded, vocab_size, max_length):\n",
    "        repeated_context = RepeatVector(max_length)(encoded)\n",
    "    \n",
    "        h = LSTM(500, return_sequences=True, name='dec_lstm_1')(repeated_context)\n",
    "        h = LSTM(500, return_sequences=True, name='dec_lstm_2')(h)\n",
    "    \n",
    "        decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)\n",
    "    \n",
    "        return decoded\n",
    "\n",
    "    def _build_sentiment_predictor(self, encoded):\n",
    "        h = Dense(100, activation='linear')(encoded)\n",
    "    \n",
    "        return Dense(1, activation='sigmoid', name='pred')(h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# from model import VAE\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 300\n",
    "NUM_WORDS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "(25000,)\n",
      "(25000,)\n",
      "Number of words:\n",
      "998\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS)\n",
    "\n",
    "print(\"Training data\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"Number of words:\")\n",
    "print(len(np.unique(np.hstack(X_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=MAX_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_LENGTH)\n",
    "\n",
    "train_indices = np.random.choice(np.arange(X_train.shape[0]), 2000, replace=False)\n",
    "test_indices = np.random.choice(np.arange(X_test.shape[0]), 1000, replace=False)\n",
    "\n",
    "X_train = X_train[train_indices]\n",
    "y_train = y_train[train_indices]\n",
    "\n",
    "X_test = X_test[test_indices]\n",
    "y_test = y_test[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.zeros((X_train.shape[0], MAX_LENGTH, NUM_WORDS))\n",
    "temp[np.expand_dims(np.arange(X_train.shape[0]), axis=0).reshape(X_train.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_train.shape[0], axis=0), X_train] = 1\n",
    "\n",
    "X_train_one_hot = temp\n",
    "\n",
    "temp = np.zeros((X_test.shape[0], MAX_LENGTH, NUM_WORDS))\n",
    "temp[np.expand_dims(np.arange(X_test.shape[0]), axis=0).reshape(X_test.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_test.shape[0], axis=0), X_test] = 1\n",
    "\n",
    "x_test_one_hot = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_checkpoint(dir, model_name):\n",
    "    filepath = dir + '/' + \\\n",
    "               model_name + \"-{epoch:02d}-{val_decoded_mean_acc:.2f}-{val_pred_loss:.2f}.h5\"\n",
    "    directory = os.path.dirname(filepath)\n",
    "\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath,\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=False)\n",
    "\n",
    "    return checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = VAE()\n",
    "    model.create(vocab_size=NUM_WORDS, max_length=MAX_LENGTH)\n",
    "\n",
    "    checkpointer = create_model_checkpoint('models', 'rnn_ae')\n",
    "\n",
    "    model.autoencoder.fit(x=X_train, y={'decoded_mean': X_train_one_hot, 'pred': y_train},\n",
    "                          batch_size=10, epochs=10, callbacks=[checkpointer],\n",
    "                          validation_data=(X_test, {'decoded_mean': x_test_one_hot, 'pred':  y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 4053s 2s/step - loss: 2.1551 - decoded_mean_loss: 1.4606 - pred_loss: 0.6946 - decoded_mean_acc: 0.3417 - pred_acc: 0.5340 - val_loss: 2.0256 - val_decoded_mean_loss: 1.3859 - val_pred_loss: 0.6397 - val_decoded_mean_acc: 0.3788 - val_pred_acc: 0.6310\n",
      "\n",
      "Epoch 00001: saving model to models/rnn_ae-01-0.38-0.64.h5\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 3964s 2s/step - loss: 1.9697 - decoded_mean_loss: 1.4117 - pred_loss: 0.5581 - decoded_mean_acc: 0.3519 - pred_acc: 0.7165 - val_loss: 1.9938 - val_decoded_mean_loss: 1.3888 - val_pred_loss: 0.6050 - val_decoded_mean_acc: 0.3788 - val_pred_acc: 0.6620\n",
      "\n",
      "Epoch 00002: saving model to models/rnn_ae-02-0.38-0.60.h5\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 3974s 2s/step - loss: 1.8889 - decoded_mean_loss: 1.4372 - pred_loss: 0.4517 - decoded_mean_acc: 0.3491 - pred_acc: 0.7995 - val_loss: 1.9430 - val_decoded_mean_loss: 1.3852 - val_pred_loss: 0.5578 - val_decoded_mean_acc: 0.3788 - val_pred_acc: 0.7180\n",
      "\n",
      "Epoch 00003: saving model to models/rnn_ae-03-0.38-0.56.h5\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 3951s 2s/step - loss: 2.1165 - decoded_mean_loss: 1.4110 - pred_loss: 0.7055 - decoded_mean_acc: 0.3466 - pred_acc: 0.5160 - val_loss: 2.0667 - val_decoded_mean_loss: 1.3720 - val_pred_loss: 0.6947 - val_decoded_mean_acc: 0.3788 - val_pred_acc: 0.4780\n",
      "\n",
      "Epoch 00004: saving model to models/rnn_ae-04-0.38-0.69.h5\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 3975s 2s/step - loss: 2.0672 - decoded_mean_loss: 1.3956 - pred_loss: 0.6716 - decoded_mean_acc: 0.3636 - pred_acc: 0.5420 - val_loss: 2.0556 - val_decoded_mean_loss: 1.3818 - val_pred_loss: 0.6737 - val_decoded_mean_acc: 0.3788 - val_pred_acc: 0.6250\n",
      "\n",
      "Epoch 00005: saving model to models/rnn_ae-05-0.38-0.67.h5\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 3974s 2s/step - loss: 2.0035 - decoded_mean_loss: 1.3940 - pred_loss: 0.6095 - decoded_mean_acc: 0.3759 - pred_acc: 0.6615 - val_loss: 2.0050 - val_decoded_mean_loss: 1.3495 - val_pred_loss: 0.6555 - val_decoded_mean_acc: 0.4212 - val_pred_acc: 0.6200\n",
      "\n",
      "Epoch 00006: saving model to models/rnn_ae-06-0.42-0.66.h5\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 3956s 2s/step - loss: 1.9896 - decoded_mean_loss: 1.3769 - pred_loss: 0.6126 - decoded_mean_acc: 0.3992 - pred_acc: 0.6575 - val_loss: 2.0004 - val_decoded_mean_loss: 1.3426 - val_pred_loss: 0.6578 - val_decoded_mean_acc: 0.4223 - val_pred_acc: 0.6210\n",
      "\n",
      "Epoch 00007: saving model to models/rnn_ae-07-0.42-0.66.h5\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 4018s 2s/step - loss: 1.9906 - decoded_mean_loss: 1.3803 - pred_loss: 0.6103 - decoded_mean_acc: 0.3871 - pred_acc: 0.6570 - val_loss: 2.0095 - val_decoded_mean_loss: 1.3454 - val_pred_loss: 0.6640 - val_decoded_mean_acc: 0.4227 - val_pred_acc: 0.6190\n",
      "\n",
      "Epoch 00008: saving model to models/rnn_ae-08-0.42-0.66.h5\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 3984s 2s/step - loss: 1.9852 - decoded_mean_loss: 1.3721 - pred_loss: 0.6131 - decoded_mean_acc: 0.4022 - pred_acc: 0.6545 - val_loss: 1.9997 - val_decoded_mean_loss: 1.3352 - val_pred_loss: 0.6644 - val_decoded_mean_acc: 0.4234 - val_pred_acc: 0.6200\n",
      "\n",
      "Epoch 00009: saving model to models/rnn_ae-09-0.42-0.66.h5\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 5045s 3s/step - loss: 1.9788 - decoded_mean_loss: 1.3660 - pred_loss: 0.6127 - decoded_mean_acc: 0.4018 - pred_acc: 0.6545 - val_loss: 1.9959 - val_decoded_mean_loss: 1.3352 - val_pred_loss: 0.6607 - val_decoded_mean_acc: 0.4232 - val_pred_acc: 0.6200\n",
      "\n",
      "Epoch 00010: saving model to models/rnn_ae-10-0.42-0.66.h5\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "model.create(vocab_size=NUM_WORDS, max_length=MAX_LENGTH)\n",
    "model.autoencoder.load_weights('models/rnn_ae-10-0.42-0.66.h5')\n",
    "encoded = model.encoder.predict(X_train[np.newaxis, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48782405]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sentiment_predictor.predict(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00099968, 0.00100005, 0.00099999, ..., 0.00100016,\n",
       "         0.00100017, 0.00099951],\n",
       "        [0.00099921, 0.00100003, 0.00099999, ..., 0.00100045,\n",
       "         0.00100037, 0.00099879],\n",
       "        [0.00099868, 0.00099993, 0.00099999, ..., 0.00100083,\n",
       "         0.00100054, 0.00099798],\n",
       "        ...,\n",
       "        [0.0009954 , 0.00099747, 0.00100081, ..., 0.00100491,\n",
       "         0.001     , 0.00099422],\n",
       "        [0.0009954 , 0.00099747, 0.00100081, ..., 0.00100491,\n",
       "         0.001     , 0.00099422],\n",
       "        [0.0009954 , 0.00099747, 0.00100081, ..., 0.00100491,\n",
       "         0.001     , 0.00099422]]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = model.decoder.predict(encoded)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, 59,  0,  0,  0, 59, 41, 21,  0, 27, 41,  0, 59, 21,  4,\n",
       "         6, 16, 59,  0,  0,  0,  0, 20,  0,  0,  5, 60,  1, 11, 59,  0,\n",
       "        51,  0, 41,  3, 22,  0,  0, 58,  0,  0, 25,  4, 60,  6,  0,  3,\n",
       "         2,  0, 26, 49,  0,  0,  0, 61, 51, 16,  0, 49, 27,  1, 59,  0,\n",
       "        41, 21,  0,  1,  0,  1,  0, 45,  0, 26, 30, 36, 49, 25,  8, 32,\n",
       "         3,  9, 17,  0, 11,  0,  0,  0, 50, 12,  0, 58,  0, 19, 79,  0,\n",
       "        54, 49,  5, 11,  0,  0, 10, 32,  3, 39, 20, 61,  0, 22,  0,  0,\n",
       "         0,  0, 58, 59, 35, 25, 61, 23, 21, 56,  4, 56, 61,  0,  5,  0,\n",
       "         0,  0, 14,  0,  0, 60,  1,  0,  1,  0,  0,  0,  0, 49,  0,  0,\n",
       "         4,  0, 59,  3,  5,  0, 50, 57, 19, 37,  0, 50,  2,  0,  0, 53,\n",
       "         0,  2, 22,  0,  0,  0, 55,  0,  2,  0,  1,  5,  0, 26,  7,  0,\n",
       "         0,  2,  0, 21, 60, 45,  0, 10,  0,  0,  0,  0,  0,  0,  0, 60,\n",
       "         0, 13,  9, 34, 47, 25,  0, 11, 60, 36, 13,  0, 20,  0,  2, 15,\n",
       "         0, 61, 17,  0,  8, 20,  0,  7, 16, 25,  0, 32, 49, 51,  0, 56,\n",
       "         0, 28,  0, 49,  0,  0,  6, 37, 36,  0, 19,  0,  4,  0,  0,  8,\n",
       "         4, 78,  0,  0,  0,  0, 35, 25, 49, 13,  8,  0,  0,  0,  6, 32,\n",
       "        55, 56,  1,  0,  0, 37, 24, 58, 49,  0,  0,  2, 40,  0, 28, 60,\n",
       "         8,  0,  0, 31,  0,  0,  0, 21, 25,  4, 57,  0, 11, 70, 20,  0,\n",
       "         0, 21,  0, 38, 54,  1,  0,  4,  8, 49,  0,  2,  2,  0, 47,  3,\n",
       "        20,  0, 45, 12, 79, 11, 53, 60,  0, 29,  0,  0, 49,  0,  8,  0,\n",
       "         0,  0, 26,  0,  2, 32, 27,  0, 58,  0,  0,  5,  9, 56,  0,  0,\n",
       "         2,  1,  2, 54,  0,  0, 28, 40, 53, 15,  4,  0,  9,  0,  0,  0,\n",
       "         0,  1,  0,  4, 24,  6, 29,  0,  0,  0,  0,  1, 39,  0, 45,  0,\n",
       "         0,  0,  0,  2,  0, 40, 24,  0, 52,  0,  0,  0,  4,  0, 32, 70,\n",
       "        40, 30,  0,  0, 14,  4, 32,  0,  0, 27,  0, 36,  0, 60, 60,  0,\n",
       "        29,  0, 41,  0,  0, 21, 27,  0,  4, 25, 60, 13,  0,  0,  0, 55,\n",
       "         0, 12,  0, 20, 38, 59,  0,  0, 30, 19, 54, 16, 17, 36,  0,  0,\n",
       "        32,  0,  0,  0, 58, 24,  0, 49,  0, 32, 57,  4,  0, 21, 45,  0,\n",
       "         0,  0,  0,  3, 30,  0,  0, 59,  8,  0,  0, 60,  0,  0,  0, 61,\n",
       "         0, 27,  0,  0, 49,  0,  0, 53, 57,  0, 19, 40,  2, 11, 23, 49,\n",
       "        51,  7, 58, 72, 40,  0, 49, 60,  0, 58, 59, 19,  0,  3,  0,  0,\n",
       "        53,  8,  0, 50, 12, 14,  0,  0, 10,  0,  1,  9, 36,  0,  0, 20,\n",
       "        45, 59,  0,  2, 27,  8, 61,  0,  0,  0,  0,  6,  0,  0,  0, 24,\n",
       "        18,  9,  0,  0,  5,  1, 19,  0,  0, 45, 30,  4, 24,  5, 12,  8,\n",
       "         2,  0,  4,  2, 26, 67, 18,  0,  0,  2, 10, 45,  0,  2,  0,  0,\n",
       "        45, 10, 45,  0,  0,  8,  0,  0,  0,  0, 21,  0, 35,  0,  0, 60,\n",
       "        35,  0, 55,  0, 32,  0,  0,  0,  6, 67,  0, 34,  0,  7,  0,  1,\n",
       "        19,  0,  5,  0, 32, 35, 11,  5,  0,  0, 45, 56,  0,  0,  0, 57,\n",
       "        56,  9, 10,  0,  0, 60, 45, 49,  0, 21, 56,  0, 57, 10, 17, 49,\n",
       "        22, 34,  4,  0,  6, 45, 22,  0,  0,  0,  0,  0,  2, 41,  0, 10,\n",
       "         3, 37,  2,  0,  0, 18,  0,  0, 16,  0, 17, 35,  0, 35, 16,  0,\n",
       "        67, 58, 27,  0,  0,  5,  0, 58,  0, 30,  6,  1,  0,  0, 57, 45,\n",
       "        30,  3, 40, 58, 58,  8,  0, 15,  6,  0,  0,  0,  0,  0, 30, 17,\n",
       "         0,  2, 58,  0,  0, 23,  0, 61, 55, 50,  2,  0,  0, 21,  0, 13,\n",
       "         0, 32, 60, 22, 59, 21,  0, 49, 17, 14, 49, 67,  4,  0,  0,  0,\n",
       "        27,  0,  0, 19, 47, 17, 36,  0, 60, 36, 59, 35,  0,  0, 11, 54,\n",
       "         0, 11, 30, 68,  0,  0, 12, 51,  0,  0,  1, 13,  0, 10, 59, 30,\n",
       "         0,  0, 29, 61,  0, 58,  0, 58, 18,  0,  0,  9, 15,  0,  0, 36,\n",
       "        58, 28, 89,  3, 55, 55, 10,  0,  0,  0,  3, 36,  0, 49,  0, 59,\n",
       "         0,  0,  0,  0, 60,  0, 56,  0,  0,  0,  8, 36, 22, 11, 27,  0,\n",
       "        24,  0,  0, 26, 56, 52, 70, 18,  3,  8, 30,  7,  0,  4, 60,  8,\n",
       "        47, 37, 49,  0,  0,  0,  2,  2, 74, 19,  0,  0, 32,  0,  6, 24,\n",
       "         1,  2,  0, 61, 57, 22,  4, 41, 11,  0,  8,  5,  1, 40, 13, 18,\n",
       "        35, 14,  7,  0,  0, 58, 29,  0,  0,  0,  0, 60,  0,  0, 19, 40,\n",
       "         0,  0, 14, 24,  0, 19, 56,  7, 16, 59, 61, 49,  0,  0,  0, 58,\n",
       "         0,  0, 83,  2, 45,  0,  2, 58,  0,  2,  0,  0,  3,  0, 28, 47,\n",
       "        15,  4,  0,  0,  0,  0,  0,  0, 19,  0,  9, 11,  0,  0,  0, 11,\n",
       "        45, 49, 23, 36, 37,  0,  0,  3, 16,  0, 56,  0, 56,  0,  0, 41,\n",
       "        21,  0, 32,  6,  0, 60,  0,  0, 26, 39,  0, 49,  0,  0,  0, 10,\n",
       "         0,  5,  6, 10,  0,  0,  0,  0, 17,  0,  0,  0,  5, 16,  0,  0,\n",
       "        30, 18,  0,  0, 37, 28,  0,  0, 18, 21, 57, 10,  0, 30,  0, 45,\n",
       "        50,  0, 52,  0,  0,  0, 39,  0,  2, 50, 49, 19,  0, 59, 36, 21,\n",
       "         7, 35,  0,  0,  0, 28,  5,  0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(decoded, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
