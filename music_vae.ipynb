{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras import objectives, backend as K\n",
    "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed\n",
    "from keras.models import Model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(object):\n",
    "    def __init__(self, vocab_size=500, max_length=300, latent_rep_size=200):\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.autoencoder = None\n",
    "\n",
    "        x = Input(shape=(max_length,))\n",
    "        x_embed = Embedding(vocab_size, 64, input_length=max_length)(x)\n",
    "\n",
    "        vae_loss, encoded = self._build_encoder(x_embed, latent_rep_size=latent_rep_size, max_length=max_length)\n",
    "        self.encoder = Model(inputs=x, outputs=encoded)\n",
    "\n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "\n",
    "        decoded = self._build_decoder(encoded_input, vocab_size, max_length)\n",
    "        self.decoder = Model(encoded_input, decoded)\n",
    "\n",
    "        self.autoencoder = Model(inputs=x, outputs=self._build_decoder(encoded, vocab_size, max_length))\n",
    "        self.autoencoder.compile(optimizer='Adam',\n",
    "                                 loss=vae_loss,\n",
    "                                 metrics=['accuracy'])\n",
    "        \n",
    "    def _build_encoder(self, x, latent_rep_size=200, max_length=300, epsilon_std=0.01):\n",
    "        h = Bidirectional(LSTM(500, return_sequences=True, name='lstm_1'), merge_mode='concat')(x)\n",
    "        h = Bidirectional(LSTM(500, return_sequences=False, name='lstm_2'), merge_mode='concat')(h)\n",
    "        h = Dense(435, activation='relu', name='dense_1')(h)\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_rep_size), mean=0., stddev=epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation='linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation='linear')(h)\n",
    "    \n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')([z_mean, z_log_var]))\n",
    "\n",
    "    def _build_decoder(self, encoded, vocab_size, max_length):\n",
    "        repeated_context = RepeatVector(max_length)(encoded)\n",
    "    \n",
    "        h = LSTM(500, return_sequences=True, name='dec_lstm_1')(repeated_context)\n",
    "        h = LSTM(500, return_sequences=True, name='dec_lstm_2')(h)\n",
    "    \n",
    "        decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)\n",
    "    \n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# from model import VAE\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 300\n",
    "NUM_WORDS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy\n",
    "from music21 import converter, instrument, note, chord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
    "    notes = []\n",
    "    songs = []\n",
    "\n",
    "    for file in glob.glob(\"midi_songs/*.mid\"):\n",
    "        song = []\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "        print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                song.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                song.append('.'.join(str(n) for n in element.normalOrder))\n",
    "        songs.append(song)\n",
    "        notes += song\n",
    "\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "\n",
    "    return notes, songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 100\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "     # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    # network_input = network_input / float(n_vocab)\n",
    "\n",
    "    # network_input = np_utils.to_categorical(network_input)\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing midi_songs/bwv782.mid\n",
      "Parsing midi_songs/bwv783.mid\n",
      "Parsing midi_songs/bwv781.mid\n",
      "Parsing midi_songs/bwv780.mid\n",
      "Parsing midi_songs/bwv784.mid\n",
      "Parsing midi_songs/bwv785.mid\n",
      "Parsing midi_songs/bwv778.mid\n",
      "Parsing midi_songs/bwv786.mid\n",
      "Parsing midi_songs/bwv779.mid\n",
      "Parsing midi_songs/bwv774.mid\n",
      "Parsing midi_songs/bwv775.mid\n",
      "Parsing midi_songs/bwv777.mid\n",
      "Parsing midi_songs/bwv776.mid\n",
      "Parsing midi_songs/bwv772.mid\n",
      "Parsing midi_songs/bwv773.mid\n"
     ]
    }
   ],
   "source": [
    "notes, songs = get_notes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "songs_text = [' '.join(song) for song in songs]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(songs_text)\n",
    "note2code = tokenizer.word_index\n",
    "\n",
    "songs_codes = tokenizer.texts_to_sequences(songs_text)\n",
    "padded_songs = pad_sequences(songs_codes, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.zeros((padded_songs.shape[0], MAX_LENGTH, NUM_WORDS))\n",
    "temp[np.expand_dims(np.arange(padded_songs.shape[0]), axis=0).reshape(padded_songs.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), padded_songs.shape[0], axis=0), padded_songs] = 1\n",
    "\n",
    "songs_one_hot = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_checkpoint(dir, model_name):\n",
    "    filepath = dir + '/' + \\\n",
    "               model_name + \"-{epoch:02d}-{acc:.2f}-{loss:.2f}.h5\"\n",
    "    directory = os.path.dirname(filepath)\n",
    "\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath,\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=False)\n",
    "\n",
    "    return checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 141s 9s/step - loss: 1.9520 - acc: 0.0284\n",
      "\n",
      "Epoch 00001: saving model to models/music_vae-01-0.03-1.95.h5\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 156s 10s/step - loss: 1.5830 - acc: 0.0449\n",
      "\n",
      "Epoch 00002: saving model to models/music_vae-02-0.04-1.58.h5\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 143s 10s/step - loss: 1.5489 - acc: 0.0387\n",
      "\n",
      "Epoch 00003: saving model to models/music_vae-03-0.04-1.55.h5\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 141s 9s/step - loss: 1.5515 - acc: 0.0291\n",
      "\n",
      "Epoch 00004: saving model to models/music_vae-04-0.03-1.55.h5\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 134s 9s/step - loss: 1.5418 - acc: 0.0316\n",
      "\n",
      "Epoch 00005: saving model to models/music_vae-05-0.03-1.54.h5\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 134s 9s/step - loss: 1.5347 - acc: 0.0322\n",
      "\n",
      "Epoch 00006: saving model to models/music_vae-06-0.03-1.53.h5\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 144s 10s/step - loss: 1.5332 - acc: 0.0320\n",
      "\n",
      "Epoch 00007: saving model to models/music_vae-07-0.03-1.53.h5\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 124s 8s/step - loss: 1.5366 - acc: 0.0376\n",
      "\n",
      "Epoch 00008: saving model to models/music_vae-08-0.04-1.54.h5\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 122s 8s/step - loss: 1.5313 - acc: 0.0367\n",
      "\n",
      "Epoch 00009: saving model to models/music_vae-09-0.04-1.53.h5\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 120s 8s/step - loss: 1.5349 - acc: 0.0491\n",
      "\n",
      "Epoch 00010: saving model to models/music_vae-10-0.05-1.53.h5\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 118s 8s/step - loss: 1.5351 - acc: 0.0380\n",
      "\n",
      "Epoch 00011: saving model to models/music_vae-11-0.04-1.54.h5\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 116s 8s/step - loss: 1.5458 - acc: 0.0322\n",
      "\n",
      "Epoch 00012: saving model to models/music_vae-12-0.03-1.55.h5\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 118s 8s/step - loss: 1.5310 - acc: 0.0398\n",
      "\n",
      "Epoch 00013: saving model to models/music_vae-13-0.04-1.53.h5\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 118s 8s/step - loss: 1.5271 - acc: 0.0460\n",
      "\n",
      "Epoch 00014: saving model to models/music_vae-14-0.05-1.53.h5\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 117s 8s/step - loss: 1.5344 - acc: 0.0331\n",
      "\n",
      "Epoch 00015: saving model to models/music_vae-15-0.03-1.53.h5\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 113s 8s/step - loss: 1.6222 - acc: 0.0476\n",
      "\n",
      "Epoch 00016: saving model to models/music_vae-16-0.05-1.62.h5\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5345 - acc: 0.0480\n",
      "\n",
      "Epoch 00017: saving model to models/music_vae-17-0.05-1.53.h5\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 123s 8s/step - loss: 1.5329 - acc: 0.0360\n",
      "\n",
      "Epoch 00018: saving model to models/music_vae-18-0.04-1.53.h5\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 114s 8s/step - loss: 1.5307 - acc: 0.0407\n",
      "\n",
      "Epoch 00019: saving model to models/music_vae-19-0.04-1.53.h5\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 115s 8s/step - loss: 1.5856 - acc: 0.0436\n",
      "\n",
      "Epoch 00020: saving model to models/music_vae-20-0.04-1.59.h5\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 116s 8s/step - loss: 1.5304 - acc: 0.0382\n",
      "\n",
      "Epoch 00021: saving model to models/music_vae-21-0.04-1.53.h5\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 115s 8s/step - loss: 1.5268 - acc: 0.0491\n",
      "\n",
      "Epoch 00022: saving model to models/music_vae-22-0.05-1.53.h5\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 114s 8s/step - loss: 1.5644 - acc: 0.0396\n",
      "\n",
      "Epoch 00023: saving model to models/music_vae-23-0.04-1.56.h5\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 115s 8s/step - loss: 1.5297 - acc: 0.0256\n",
      "\n",
      "Epoch 00024: saving model to models/music_vae-24-0.03-1.53.h5\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5484 - acc: 0.0433\n",
      "\n",
      "Epoch 00025: saving model to models/music_vae-25-0.04-1.55.h5\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5340 - acc: 0.0489\n",
      "\n",
      "Epoch 00026: saving model to models/music_vae-26-0.05-1.53.h5\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5286 - acc: 0.0418\n",
      "\n",
      "Epoch 00027: saving model to models/music_vae-27-0.04-1.53.h5\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5375 - acc: 0.0413\n",
      "\n",
      "Epoch 00028: saving model to models/music_vae-28-0.04-1.54.h5\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5376 - acc: 0.0496\n",
      "\n",
      "Epoch 00029: saving model to models/music_vae-29-0.05-1.54.h5\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 117s 8s/step - loss: 1.5268 - acc: 0.0491\n",
      "\n",
      "Epoch 00030: saving model to models/music_vae-30-0.05-1.53.h5\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 113s 8s/step - loss: 1.5882 - acc: 0.0540\n",
      "\n",
      "Epoch 00031: saving model to models/music_vae-31-0.05-1.59.h5\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 110s 7s/step - loss: 1.5911 - acc: 0.0356\n",
      "\n",
      "Epoch 00032: saving model to models/music_vae-32-0.04-1.59.h5\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 110s 7s/step - loss: 1.5342 - acc: 0.0438\n",
      "\n",
      "Epoch 00033: saving model to models/music_vae-33-0.04-1.53.h5\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5273 - acc: 0.0489\n",
      "\n",
      "Epoch 00034: saving model to models/music_vae-34-0.05-1.53.h5\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5264 - acc: 0.0489\n",
      "\n",
      "Epoch 00035: saving model to models/music_vae-35-0.05-1.53.h5\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5214 - acc: 0.0453\n",
      "\n",
      "Epoch 00036: saving model to models/music_vae-36-0.05-1.52.h5\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5229 - acc: 0.0491\n",
      "\n",
      "Epoch 00037: saving model to models/music_vae-37-0.05-1.52.h5\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5217 - acc: 0.0440\n",
      "\n",
      "Epoch 00038: saving model to models/music_vae-38-0.04-1.52.h5\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5188 - acc: 0.0489\n",
      "\n",
      "Epoch 00039: saving model to models/music_vae-39-0.05-1.52.h5\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5277 - acc: 0.0462\n",
      "\n",
      "Epoch 00040: saving model to models/music_vae-40-0.05-1.53.h5\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5165 - acc: 0.0489\n",
      "\n",
      "Epoch 00041: saving model to models/music_vae-41-0.05-1.52.h5\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5182 - acc: 0.0491\n",
      "\n",
      "Epoch 00042: saving model to models/music_vae-42-0.05-1.52.h5\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5181 - acc: 0.0491\n",
      "\n",
      "Epoch 00043: saving model to models/music_vae-43-0.05-1.52.h5\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5156 - acc: 0.0491\n",
      "\n",
      "Epoch 00044: saving model to models/music_vae-44-0.05-1.52.h5\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5171 - acc: 0.0491\n",
      "\n",
      "Epoch 00045: saving model to models/music_vae-45-0.05-1.52.h5\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 116s 8s/step - loss: 1.5157 - acc: 0.0491\n",
      "\n",
      "Epoch 00046: saving model to models/music_vae-46-0.05-1.52.h5\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 113s 8s/step - loss: 1.5157 - acc: 0.0491\n",
      "\n",
      "Epoch 00047: saving model to models/music_vae-47-0.05-1.52.h5\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5153 - acc: 0.0491\n",
      "\n",
      "Epoch 00048: saving model to models/music_vae-48-0.05-1.52.h5\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5150 - acc: 0.0491\n",
      "\n",
      "Epoch 00049: saving model to models/music_vae-49-0.05-1.51.h5\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5184 - acc: 0.0491\n",
      "\n",
      "Epoch 00050: saving model to models/music_vae-50-0.05-1.52.h5\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5155 - acc: 0.0491\n",
      "\n",
      "Epoch 00051: saving model to models/music_vae-51-0.05-1.52.h5\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 111s 7s/step - loss: 1.5148 - acc: 0.0491\n",
      "\n",
      "Epoch 00052: saving model to models/music_vae-52-0.05-1.51.h5\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5161 - acc: 0.0491\n",
      "\n",
      "Epoch 00053: saving model to models/music_vae-53-0.05-1.52.h5\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 113s 8s/step - loss: 1.5143 - acc: 0.0491\n",
      "\n",
      "Epoch 00054: saving model to models/music_vae-54-0.05-1.51.h5\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 113s 8s/step - loss: 1.5145 - acc: 0.0491\n",
      "\n",
      "Epoch 00055: saving model to models/music_vae-55-0.05-1.51.h5\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5131 - acc: 0.0491\n",
      "\n",
      "Epoch 00056: saving model to models/music_vae-56-0.05-1.51.h5\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5131 - acc: 0.0491\n",
      "\n",
      "Epoch 00057: saving model to models/music_vae-57-0.05-1.51.h5\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5142 - acc: 0.0491\n",
      "\n",
      "Epoch 00058: saving model to models/music_vae-58-0.05-1.51.h5\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5139 - acc: 0.0491\n",
      "\n",
      "Epoch 00059: saving model to models/music_vae-59-0.05-1.51.h5\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5142 - acc: 0.0491\n",
      "\n",
      "Epoch 00060: saving model to models/music_vae-60-0.05-1.51.h5\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5135 - acc: 0.0491\n",
      "\n",
      "Epoch 00061: saving model to models/music_vae-61-0.05-1.51.h5\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5140 - acc: 0.0491\n",
      "\n",
      "Epoch 00062: saving model to models/music_vae-62-0.05-1.51.h5\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 117s 8s/step - loss: 1.5143 - acc: 0.0491\n",
      "\n",
      "Epoch 00063: saving model to models/music_vae-63-0.05-1.51.h5\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5129 - acc: 0.0491\n",
      "\n",
      "Epoch 00064: saving model to models/music_vae-64-0.05-1.51.h5\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 114s 8s/step - loss: 1.5131 - acc: 0.0493\n",
      "\n",
      "Epoch 00065: saving model to models/music_vae-65-0.05-1.51.h5\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 113s 8s/step - loss: 1.5147 - acc: 0.0493\n",
      "\n",
      "Epoch 00066: saving model to models/music_vae-66-0.05-1.51.h5\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 110s 7s/step - loss: 1.5140 - acc: 0.0493\n",
      "\n",
      "Epoch 00067: saving model to models/music_vae-67-0.05-1.51.h5\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5134 - acc: 0.0493\n",
      "\n",
      "Epoch 00068: saving model to models/music_vae-68-0.05-1.51.h5\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5134 - acc: 0.0493\n",
      "\n",
      "Epoch 00069: saving model to models/music_vae-69-0.05-1.51.h5\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5131 - acc: 0.0493\n",
      "\n",
      "Epoch 00070: saving model to models/music_vae-70-0.05-1.51.h5\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5141 - acc: 0.0493\n",
      "\n",
      "Epoch 00071: saving model to models/music_vae-71-0.05-1.51.h5\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 116s 8s/step - loss: 1.5115 - acc: 0.0493\n",
      "\n",
      "Epoch 00072: saving model to models/music_vae-72-0.05-1.51.h5\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 113s 8s/step - loss: 1.5133 - acc: 0.0493\n",
      "\n",
      "Epoch 00073: saving model to models/music_vae-73-0.05-1.51.h5\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5207 - acc: 0.0493\n",
      "\n",
      "Epoch 00074: saving model to models/music_vae-74-0.05-1.52.h5\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5167 - acc: 0.0491\n",
      "\n",
      "Epoch 00075: saving model to models/music_vae-75-0.05-1.52.h5\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5149 - acc: 0.0491\n",
      "\n",
      "Epoch 00076: saving model to models/music_vae-76-0.05-1.51.h5\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5161 - acc: 0.0480\n",
      "\n",
      "Epoch 00077: saving model to models/music_vae-77-0.05-1.52.h5\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 113s 8s/step - loss: 1.5130 - acc: 0.0489\n",
      "\n",
      "Epoch 00078: saving model to models/music_vae-78-0.05-1.51.h5\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 118s 8s/step - loss: 1.5140 - acc: 0.0491\n",
      "\n",
      "Epoch 00079: saving model to models/music_vae-79-0.05-1.51.h5\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5141 - acc: 0.0491\n",
      "\n",
      "Epoch 00080: saving model to models/music_vae-80-0.05-1.51.h5\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5133 - acc: 0.0489\n",
      "\n",
      "Epoch 00081: saving model to models/music_vae-81-0.05-1.51.h5\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5140 - acc: 0.0491\n",
      "\n",
      "Epoch 00082: saving model to models/music_vae-82-0.05-1.51.h5\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5131 - acc: 0.0489\n",
      "\n",
      "Epoch 00083: saving model to models/music_vae-83-0.05-1.51.h5\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5136 - acc: 0.0489\n",
      "\n",
      "Epoch 00084: saving model to models/music_vae-84-0.05-1.51.h5\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5132 - acc: 0.0491\n",
      "\n",
      "Epoch 00085: saving model to models/music_vae-85-0.05-1.51.h5\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5102 - acc: 0.0516\n",
      "\n",
      "Epoch 00086: saving model to models/music_vae-86-0.05-1.51.h5\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5162 - acc: 0.0496\n",
      "\n",
      "Epoch 00087: saving model to models/music_vae-87-0.05-1.52.h5\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5138 - acc: 0.0500\n",
      "\n",
      "Epoch 00088: saving model to models/music_vae-88-0.05-1.51.h5\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5126 - acc: 0.0500\n",
      "\n",
      "Epoch 00089: saving model to models/music_vae-89-0.05-1.51.h5\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5078 - acc: 0.0522\n",
      "\n",
      "Epoch 00090: saving model to models/music_vae-90-0.05-1.51.h5\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5115 - acc: 0.0502\n",
      "\n",
      "Epoch 00091: saving model to models/music_vae-91-0.05-1.51.h5\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5144 - acc: 0.0496\n",
      "\n",
      "Epoch 00092: saving model to models/music_vae-92-0.05-1.51.h5\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5392 - acc: 0.0431\n",
      "\n",
      "Epoch 00093: saving model to models/music_vae-93-0.04-1.54.h5\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5302 - acc: 0.0367\n",
      "\n",
      "Epoch 00094: saving model to models/music_vae-94-0.04-1.53.h5\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 113s 8s/step - loss: 1.6910 - acc: 0.0393\n",
      "\n",
      "Epoch 00095: saving model to models/music_vae-95-0.04-1.69.h5\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 117s 8s/step - loss: 1.5273 - acc: 0.0498\n",
      "\n",
      "Epoch 00096: saving model to models/music_vae-96-0.05-1.53.h5\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 112s 7s/step - loss: 1.5160 - acc: 0.0498\n",
      "\n",
      "Epoch 00097: saving model to models/music_vae-97-0.05-1.52.h5\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 113s 8s/step - loss: 1.5163 - acc: 0.0496\n",
      "\n",
      "Epoch 00098: saving model to models/music_vae-98-0.05-1.52.h5\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 111s 7s/step - loss: 1.5181 - acc: 0.0496\n",
      "\n",
      "Epoch 00099: saving model to models/music_vae-99-0.05-1.52.h5\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 113s 8s/step - loss: 1.5141 - acc: 0.0491\n",
      "\n",
      "Epoch 00100: saving model to models/music_vae-100-0.05-1.51.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x128684eb8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE(vocab_size=NUM_WORDS, max_length=MAX_LENGTH)\n",
    "\n",
    "checkpointer = create_model_checkpoint('models', 'music_vae')\n",
    "\n",
    "model.autoencoder.fit(x=padded_songs, y=songs_one_hot,\n",
    "                      batch_size=1, epochs=100, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_output = model.autoencoder.predict(padded_songs[np.newaxis, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A3', 'E4', 'E4', 'E4', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5', 'D5']\n"
     ]
    }
   ],
   "source": [
    "prediction_indices = np.argmax(prediction_output, axis=2)\n",
    "code2note = dict([[code, note] for note, code in note2code.items()])\n",
    "\n",
    "prediction_song = [code2note[index] for index in prediction_indices[0]]\n",
    "print(prediction_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import instrument, note, stream, chord\n",
    "\n",
    "def create_midi(prediction_output, file_path):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_midi(prediction_song, 'test_vae_out.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
